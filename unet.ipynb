{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Problem 1] Learning / Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"D:/competition_data/competition_data/train/images\"\n",
    "mask_folder = \"D:/competition_data/competition_data/train/masks\"\n",
    "\n",
    "image_files = os.listdir(image_folder)\n",
    "mask_files = os.listdir(mask_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "\n",
    "def preprocess_image(img, target_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "  img = cv2.resize(img, target_size)\n",
    "  img = img / 255.0  \n",
    "  return img\n",
    "\n",
    "\n",
    "def load_images_and_masks(image_folder, mask_folder, image_files):\n",
    "  images = []\n",
    "  masks = []\n",
    "  \n",
    "  for image_file in image_files:\n",
    "    image = cv2.imread(os.path.join(image_folder, image_file), cv2.IMREAD_GRAYSCALE)\n",
    "    image = preprocess_image(image)\n",
    "\n",
    "  \n",
    "    mask = cv2.imread(os.path.join(mask_folder, image_file), cv2.IMREAD_GRAYSCALE)\n",
    "    mask = preprocess_image(mask)\n",
    "\n",
    "    images.append(image)\n",
    "    masks.append(mask)\n",
    "  \n",
    "  return np.array(images), np.array(masks)\n",
    "\n",
    "images, masks = load_images_and_masks(image_folder, mask_folder, image_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=10, zoom_range=0.2, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(input_size=(128, 128, 1)):\n",
    "  inputs = layers.Input(input_size)\n",
    "  \n",
    "  c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "  c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "  p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "  c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "  c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "  p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "  c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "  c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "  u2 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c3)\n",
    "  u2 = layers.concatenate([u2, c2])\n",
    "  c4 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u2)\n",
    "  c4 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "  u1 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c4)\n",
    "  u1 = layers.concatenate([u1, c1])\n",
    "  c5 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u1)\n",
    "  c5 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "  outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
    "  \n",
    "  model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "  return model\n",
    "\n",
    "model = unet_model()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 9s/step - accuracy: 0.7271 - loss: 0.5815 - val_accuracy: 0.7524 - val_loss: 0.5507\n",
      "Epoch 2/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1043s\u001b[0m 10s/step - accuracy: 0.7444 - loss: 0.5584 - val_accuracy: 0.7524 - val_loss: 0.4663\n",
      "Epoch 3/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1024s\u001b[0m 10s/step - accuracy: 0.7592 - loss: 0.4712 - val_accuracy: 0.8196 - val_loss: 0.4470\n",
      "Epoch 4/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1426s\u001b[0m 14s/step - accuracy: 0.8131 - loss: 0.4343 - val_accuracy: 0.8446 - val_loss: 0.3787\n",
      "Epoch 5/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1012s\u001b[0m 10s/step - accuracy: 0.8275 - loss: 0.3994 - val_accuracy: 0.8522 - val_loss: 0.3551\n",
      "Epoch 6/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1041s\u001b[0m 10s/step - accuracy: 0.8423 - loss: 0.3743 - val_accuracy: 0.8708 - val_loss: 0.3215\n",
      "Epoch 7/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m980s\u001b[0m 10s/step - accuracy: 0.8659 - loss: 0.3363 - val_accuracy: 0.8781 - val_loss: 0.2980\n",
      "Epoch 8/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1168s\u001b[0m 12s/step - accuracy: 0.8646 - loss: 0.3332 - val_accuracy: 0.8942 - val_loss: 0.2810\n",
      "Epoch 9/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1209s\u001b[0m 12s/step - accuracy: 0.8827 - loss: 0.2979 - val_accuracy: 0.8829 - val_loss: 0.3137\n",
      "Epoch 10/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1052s\u001b[0m 10s/step - accuracy: 0.8810 - loss: 0.3027 - val_accuracy: 0.8990 - val_loss: 0.2594\n",
      "Epoch 11/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m909s\u001b[0m 9s/step - accuracy: 0.8921 - loss: 0.2715 - val_accuracy: 0.9023 - val_loss: 0.2487\n",
      "Epoch 12/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m944s\u001b[0m 9s/step - accuracy: 0.8923 - loss: 0.2668 - val_accuracy: 0.8991 - val_loss: 0.2546\n",
      "Epoch 13/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1273s\u001b[0m 13s/step - accuracy: 0.8953 - loss: 0.2650 - val_accuracy: 0.9051 - val_loss: 0.2412\n",
      "Epoch 14/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m943s\u001b[0m 9s/step - accuracy: 0.8962 - loss: 0.2586 - val_accuracy: 0.8969 - val_loss: 0.2552\n",
      "Epoch 15/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m967s\u001b[0m 10s/step - accuracy: 0.8996 - loss: 0.2545 - val_accuracy: 0.9083 - val_loss: 0.2311\n",
      "Epoch 16/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1697s\u001b[0m 17s/step - accuracy: 0.9079 - loss: 0.2300 - val_accuracy: 0.9150 - val_loss: 0.2220\n",
      "Epoch 17/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m973s\u001b[0m 10s/step - accuracy: 0.9066 - loss: 0.2316 - val_accuracy: 0.9175 - val_loss: 0.2128\n",
      "Epoch 18/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1003s\u001b[0m 10s/step - accuracy: 0.9085 - loss: 0.2283 - val_accuracy: 0.9062 - val_loss: 0.2387\n",
      "Epoch 19/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1494s\u001b[0m 15s/step - accuracy: 0.9117 - loss: 0.2160 - val_accuracy: 0.9236 - val_loss: 0.1938\n",
      "Epoch 20/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 5s/step - accuracy: 0.9157 - loss: 0.2073 - val_accuracy: 0.9230 - val_loss: 0.2002\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.9264 - loss: 0.1910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20022931694984436, 0.9230294227600098]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Problem 2] Code reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code fourni est destiné à la segmentation d'images, typiquement utilisée dans des tâches comme la détection d'objets ou la segmentation sémantique pour des classes multiples ou binaires. Voici une explication des différentes parties du code :\n",
    "\n",
    "1- data.py\n",
    "  Les premières lignes définissent différentes classes d'objets (ciel, bâtiment, poteau, route, etc.) représentées par des valeurs de couleur spécifiques en tant que listes RGB. Ces couleurs sont stockées dans un tableau numpy (`COLOR_DICT`).\n",
    "\n",
    "  La fonction `adjustData` prend en entrée une image et un masque (qui représente les annotations des classes sur l'image), et effectue une normalisation ou une binarisation selon le cas \n",
    "\n",
    "  La fonction `trainGenerator` génère par lots des images et leurs masques associés pour l'entraînement du modèle, en appliquant des augmentations de données (rotations, inversions, etc.) sur les images et les masques de manière synchronisée pour garantir que les transformations sont identiques sur les deux. \n",
    "\n",
    "  La fonction `testGenerator` génère des images à partir du répertoire de test. Elle charge une image, la redimensionne et la normalise avant de la transformer en un format compatible avec le modèle (ajoute des dimensions si nécessaire)\n",
    "\n",
    "  la fonction `geneTrainNpy` permet de charger un ensemble d'images et de masques à partir de fichiers `.png`, puis de les ajuster en utilisant la fonction `adjustData`. Elle retourne deux tableaux Numpy : un pour les images et un pour les masques\n",
    "\n",
    "  La fonction `labelVisualize` convertit le masque de prédiction, qui contient des valeurs de classe, en une image RGB correspondant aux couleurs définies dans `COLOR_DICT`. Pour chaque classe, la fonction applique la couleur appropriée à tous les pixels appartenant à cette classe.\n",
    "\n",
    "  la fonction `saveResult` enregistre les résultats des prédictions générées par le modèle sous forme d'images. Elle prend en entrée un tableau contenant les masques prédits et les convertit en images visuelles (RGB) à l'aide de `labelVisualize` si plusieurs classes sont présentes. Les résultats sont ensuite sauvegardés en tant que fichiers `.png`.\n",
    "\n",
    "2- main.py \n",
    "\n",
    "  Les données d'entraînement sont augmentées à l'aide de transformations aléatoires.\n",
    "\n",
    "  Un modèle U-Net est créé et entraîné sur ces données augmentées.\n",
    "\n",
    "  Les images de test sont chargées, et le modèle effectue des prédictions sur celles-ci.\n",
    "\n",
    "  Les prédictions sont sauvegardées sous forme d'images segmentées.\n",
    "\n",
    "3- model.py\n",
    "  Le modèle U-Net implémenté dans la fonction unet est un réseau convolutif pour la segmentation d'images, composé de :\n",
    "  Un chemin descendant (encodeur) pour extraire des caractéristiques globales en réduisant progressivement la résolution de l'image.\n",
    "  Un chemin montant (décodeur) pour restaurer la résolution originale tout en combinant les caractéristiques globales et locales grâce aux connexions entre les couches correspondantes de l'encodeur et du décodeur.\n",
    "  Le modèle est compilé avec l'optimiseur Adam et est conçu pour des tâches de segmentation binaire avec une fonction de perte adaptée (binary_crossentropy)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
