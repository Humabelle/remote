{"cells":[{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","import nltk\n","import re\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","import numpy as np\n"]},{"cell_type":"markdown","metadata":{},"source":["# [Problème 1] Implémentation Scratch de BoW"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sentences = [\n","  \"This movie is SOOOO funny!!!\",\n","  \"What a movie! I never\",\n","  \"best movie ever!!!!! this movie\"\n","]\n","\n","def tokenize(sentence):\n","  sentence = re.sub(r'[^\\w\\s]', '', sentence).lower()\n","  return sentence.split()\n","\n","\n","def generate_ngrams(tokens, n):\n","  return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n","\n","\n","def build_vocabulary(sentences, n):\n","  vocabulary = set()\n","  for sentence in sentences:\n","    tokens = tokenize(sentence)\n","    ngrams = generate_ngrams(tokens, n)\n","    vocabulary.update(ngrams)\n","  return sorted(list(vocabulary))\n","\n","\n","def sentence_to_bow(sentence, vocabulary, n):\n","  tokens = tokenize(sentence)\n","  ngrams = generate_ngrams(tokens, n)\n","  bow = Counter(ngrams)\n","  return [bow[term] for term in vocabulary]\n","\n","\n","def bow_representation(sentences, n):\n","  vocabulary = build_vocabulary(sentences, n)\n","  print(f\"Vocabulary ({n}-gram): {vocabulary}\")\n","  bows = []\n","  for sentence in sentences:\n","    bows.append(sentence_to_bow(sentence, vocabulary, n))\n","  return bows\n","\n","bow_1gram = bow_representation(sentences, 1)\n","bow_2gram = bow_representation(sentences, 2)\n","\n","print(\"\\nBoW 1-gram Representation:\")\n","for i, bow in enumerate(bow_1gram):\n","  print(f\"Sentence {i+1}: {bow}\")\n","\n","print(\"\\nBoW 2-gram Representation:\")\n","for i, bow in enumerate(bow_2gram):\n","  print(f\"Sentence {i+1}: {bow}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# [Problème 2] Calcul TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.datasets import load_files\n","\n","train_review = load_files('./aclImdb/train/', encoding='utf-8')\n","x_train, y_train = train_review.data, train_review.target\n","\n","test_review = load_files('./aclImdb/test/', encoding='utf-8')\n","x_test, y_test = test_review.data, test_review.target\n","\n","\n","print(train_review.target_names)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nltk.download('stopwords')\n","\n","reviews = x_train\n","\n","stop_words = list(stopwords.words('english'))\n","\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=stop_words)\n","\n","tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)\n","\n","tfidf_dense = tfidf_matrix.todense()\n","\n","print(tfidf_dense)\n","print(tfidf_vectorizer.get_feature_names_out())\n"]},{"cell_type":"markdown","metadata":{},"source":["# [Problème 3] Apprendre avec TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nltk.download('stopwords')\n","\n","stop_words = list(stopwords.words('english'))\n","\n","\n","tfidf_vectorizer = TfidfVectorizer(\n","  max_features=5000, \n","  stop_words=stop_words, \n","  ngram_range=(1, 2)\n",")\n","\n","\n","x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n","x_test_tfidf = tfidf_vectorizer.transform(x_test)\n","\n","\n","model = LogisticRegression(max_iter=200)\n","model.fit(x_train_tfidf, y_train)\n","\n","\n","y_pred = model.predict(x_test_tfidf)\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Précision : {accuracy:.4f}\")\n","\n","print(tfidf_vectorizer.get_feature_names_out())\n"]},{"cell_type":"markdown","metadata":{},"source":["# [Problème 4] Implémentation Scratch de TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import math\n","\n","documents = [\n","  \"This movie is SOOOO funny!!!\",\n","  \"What a movie! I never\",\n","  \"best movie ever!!!!! this movie\"\n","]\n","\n","\n","def tokenize(doc):\n","  return doc.lower().split()\n","\n","\n","def compute_tf(doc):\n","  tf = Counter(tokenize(doc))\n","  doc_len = len(tokenize(doc))\n","  return {term: count / doc_len for term, count in tf.items()}\n","\n","\n","def compute_idf_standard(docs):\n","  N = len(docs)\n","  idf = {}\n","  all_terms = list(term for doc in docs for term in tokenize(doc))\n","\n","  for term in all_terms:\n","    df = sum(1 for doc in docs if term in tokenize(doc))\n","    idf[term] = math.log(N / df)\n","\n","  return idf\n","\n","def compute_idf_sklearn(docs):\n","  N = len(docs)\n","  idf = {}\n","  all_terms = list(term for doc in docs for term in tokenize(doc))\n","\n","  for term in all_terms:\n","    df = sum(1 for doc in docs if term in tokenize(doc))\n","    idf[term] = math.log(N / (1 + df)) + 1\n","  return idf\n","\n","\n","def compute_tf_idf(doc, docs, idf):\n","  tf = compute_tf(doc)\n","  return {term: tf_val * idf[term] for term, tf_val in tf.items()}\n","\n","\n","def compute_tf_idf_all(docs, idf_function):\n","  idf = idf_function(docs)\n","  tf_idf_docs = []\n","\n","  for doc in docs:\n","    tf_idf = compute_tf_idf(doc, docs, idf)\n","    tf_idf_docs.append(tf_idf)\n","\n","  return tf_idf_docs\n","\n","\n","tf_idf_standard = compute_tf_idf_all(documents, compute_idf_standard)\n","\n","tf_idf_sklearn = compute_tf_idf_all(documents, compute_idf_sklearn)\n","\n","print(\"TF-IDF (Standard):\")\n","for i, doc in enumerate(tf_idf_standard):\n","  print(f\"Document {i+1}: {doc}\")\n","\n","print(\"\\nTF-IDF (scikit-learn):\")\n","for i, doc in enumerate(tf_idf_sklearn):\n","  print(f\"Document {i+1}: {doc}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# [Problème 5] Prétraitement du corpus"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nltk.download('stopwords')\n","\n","def preprocess_text(text):\n","  text = text.lower()\n","  text = re.sub(r'http\\S+', '', text)  \n","  text = re.sub(r'[^a-z\\s]', '', text)  \n","  tokens = text.split()  \n","  return tokens\n","\n","x_train_processed = [preprocess_text(review) for review in x_train]\n"]},{"cell_type":"markdown","metadata":{},"source":["# [Problème 6] Apprentissage de Word2Vec"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["from gensim.models import Word2Vec\n","\n","word2vec_model = Word2Vec(\n","  sentences=x_train_processed,\n","  vector_size=100,\n","  window=5,\n","  min_count=2,\n","  sg=0,  \n","  workers=4\n",")\n","\n","word2vec_model.train(x_train_processed, total_examples=len(x_train_processed), epochs=10)\n","\n","\n","# Sauvegarde du modèle\n","word2vec_model.save(\"word2vec_imdb.model\")"]},{"cell_type":"markdown","metadata":{},"source":["# [Problème 7] (Problème avancé) Visualisation vectorielle"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["from sklearn.manifold import TSNE\n","\n","\n","word_vectors = word2vec_model.wv.vectors\n","words = word2vec_model.wv.index_to_key\n","\n","tsne = TSNE(n_components=2, random_state=42)\n","word_vectors_2d = tsne.fit_transform(word_vectors)\n","\n","plt.figure(figsize=(10, 8))\n","plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])\n","\n","selected_words = ['good', 'bad', 'movie', 'love', 'happy', 'sad']  \n","for i, word in enumerate(words):\n","  if word in selected_words:\n","    plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["similar_words = word2vec_model.wv.most_similar('movie', topn=5)\n","print(similar_words)\n"]},{"cell_type":"markdown","metadata":{},"source":["# [Problème 8] (Problème avancé) Classification des revues de films à l'aide de Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_average_vector(tokens, model):\n","  vectors = [model.wv[token] for token in tokens if token in model.wv]\n","  if vectors:\n","    return np.mean(vectors, axis=0)\n","  else:\n","    return np.zeros(model.vector_size)\n","\n","x_train_vectors = np.array([get_average_vector(review, word2vec_model) for review in x_train_processed])\n","x_test_vectors = np.array([get_average_vector(preprocess_text(review), word2vec_model) for review in x_test])\n","\n","label_encoder = LabelEncoder()\n","y_train_encoded = label_encoder.fit_transform(y_train)\n","y_test_encoded = label_encoder.transform(y_test)\n","\n","classifier = LogisticRegression(max_iter=200)\n","classifier.fit(x_train_vectors, y_train_encoded)\n","\n","y_pred = classifier.predict(x_test_vectors)\n","\n","accuracy = accuracy_score(y_test_encoded, y_pred)\n","print(f\"Précision : {accuracy:.4f}\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":2}
